{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "appnope           0.1.4\n",
      "asttokens         2.4.1\n",
      "comm              0.2.2\n",
      "contourpy         1.2.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.1\n",
      "decorator         5.1.1\n",
      "exceptiongroup    1.2.0\n",
      "executing         2.0.1\n",
      "fonttools         4.50.0\n",
      "ipykernel         6.29.4\n",
      "ipython           8.22.2\n",
      "jedi              0.19.1\n",
      "jupyter_client    8.6.1\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.5\n",
      "matplotlib        3.8.3\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.6.0\n",
      "numpy             1.23.0\n",
      "packaging         24.0\n",
      "parso             0.8.3\n",
      "pexpect           4.9.0\n",
      "pillow            10.2.0\n",
      "pip               23.0.1\n",
      "platformdirs      4.2.0\n",
      "prompt-toolkit    3.0.43\n",
      "psutil            5.9.8\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "Pygments          2.17.2\n",
      "pyparsing         3.1.2\n",
      "python-dateutil   2.9.0.post0\n",
      "pyzmq             25.1.2\n",
      "setuptools        65.5.0\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tornado           6.4\n",
      "traitlets         5.14.2\n",
      "wcwidth           0.2.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy==1.23.0 \n",
    "# !pip install -U cupy-cuda11x\n",
    "# !pip install -U cupy-cuda12x\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "import dezero.models as M\n",
    "from dezero import cuda\n",
    "from dezero.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from dezero import cuda\n",
    "use_gpu = False\n",
    "use_gpu = cuda.gpu_enable\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "x = cp.arange(6).reshape(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import cuda\n",
    "from typing import List, Optional, Tuple, Any\n",
    "import math\n",
    "\n",
    "class StrokesDataset(dezero.DataLoader):\n",
    "    def __init__(self, data, batch_size, max_seq_length: int, scale: Optional[float] = None, shuffle=True, gpu=False):\n",
    "        stroke_data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(data)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        \n",
    "        for seq in data:\n",
    "            # we will deem a sequence that is less than 10 as too short and thus ignore it\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # clamp the delta x and delta y to [-1000, 1000]\n",
    "                seq = xp.minimum(seq, 1000)\n",
    "                seq = xp.maximum(seq, -1000)\n",
    "                \n",
    "                seq = xp.array(seq, dtype=xp.float32)\n",
    "                stroke_data.append(seq)\n",
    "        \n",
    "        if scale is None:\n",
    "            # calculate the scale factor\n",
    "            # the scale factor is the standard deviation of the x and y coordinates\n",
    "            # mean is not adjusted for simplicity\n",
    "            # 0:2 means the first two columns of the array which are x and y coordinates\n",
    "            scale = xp.std(xp.concatenate([xp.ravel(s[:,0:2]) for s in stroke_data]))\n",
    "        \n",
    "        longest_seq_len = max([len(seq) for seq in stroke_data])\n",
    "        \n",
    "        # we add two extra columns to the dataset since we currently there are only 3 columns in the dataset\n",
    "        # additional two columns are for changing the last point 1/0 to a one-hot vector\n",
    "        temp_stroke_dataset = xp.zeros((len(stroke_data), longest_seq_len + 2, 5), dtype=xp.float32)\n",
    "        \n",
    "        # self.mask is used to mark areas of the sequence that are not used\n",
    "        # we first initialize it to zero\n",
    "        temp_mask_dataset = xp.zeros((len(stroke_data), longest_seq_len + 1))\n",
    "        \n",
    "        self.dataset = []\n",
    "        \n",
    "        # start of sequence is [0, 0, 1, 0, 0]\n",
    "        \n",
    "        for i, seq in enumerate(stroke_data):\n",
    "            seq = xp.array(seq, dtype=xp.float32)\n",
    "            len_seq = len(seq)\n",
    "            \n",
    "            # we start from 1 to leave the first row for the start of sequence token\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 0:2] = seq[:, :2] / scale # this is the x and y coordinates\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 2] = 1 - seq[:, 2] # this is the pen down\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 3] = seq[:, 2] # this is the pen up\n",
    "            temp_stroke_dataset[i, len_seq + 1, 4] = 1  # this is the end of sequence token\n",
    "            temp_mask_dataset[i, :len_seq + 1] = 1 # mask is on until the end of the sequence \n",
    "            # self.mask is used to mark areas of the sequence that are not used\n",
    "            # for example, if the sequence is shorter than the longest sequence, we use mask to ignore the rest of the sequence\n",
    "            # an example of mask is [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        temp_stroke_dataset[:, 0, 2] = 1\n",
    "        \n",
    "        for i in range(len(stroke_data)):\n",
    "            self.dataset.append([temp_stroke_dataset[i], temp_mask_dataset[i]])\n",
    "        \n",
    "        \n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "\n",
    "# According to other estimates\n",
    "# the number of distributions in the mixture model is 20\n",
    "# https://github.com/Shun14/sketch-rnn-kanji\n",
    "# https://nn.labml.ai/sketch_rnn/index.html\n",
    "\n",
    "# This is for getting the loss of delta_x and delta_y\n",
    "class BivariateGaussianMixture:\n",
    "    def __init__(self, pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
    "        # check if the pi_probs sum for each sequence is 1\n",
    "        # print('test pi', test_pi.shape) # pi shape is (batch_size, seq_len, n_distributions)\n",
    "        # check if the pi probabilities sum to 1\n",
    "        # seq_len = pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(pi_logits, axis=2), (-1, seq_len)), 'sum of pi')\n",
    "        self.pi_logits = pi_logits\n",
    "        self.pi_probs = F.softmax(pi_logits, axis=2)\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "\n",
    "        xp = cuda.cupy if cuda.gpu_enable else np\n",
    "    \n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        return self.pi_logits.shape[-1]\n",
    "    \n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.pi_logits /= temperature\n",
    "        self.pi_probs = F.softmax(self.pi_logits, axis=2) # we do this to make sure the pi probabilities sum to 1\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "    \n",
    "    def gaussian_pdf(self, x_delta, y_delta):\n",
    "        # the result means the probability of y in the normal distribution\n",
    "        # we check the probability of y in the normal distribution\n",
    "        # if the probability is high, the result is close to 1\n",
    "        # x_delta and y_delta shape are (batch_size, seq_len, hidden_size)\n",
    "        # mu_x and mu_y shape are (batch_size, seq_len, n_distributions)\n",
    "        norm1 = F.sub(x_delta, self.mu_x)\n",
    "        norm2 = F.sub(y_delta, self.mu_y)\n",
    "        xp = cuda.get_array_module(norm1)\n",
    "\n",
    "        dtype = self.sigma_x.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        self.sigma_x = F.clip(self.sigma_x, 1e-5, max_dtype)\n",
    "        self.sigma_y = F.clip(self.sigma_y, 1e-5, max_dtype)\n",
    "        self.rho_xy = F.clip(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "        \n",
    "        s1s2 = F.mul(self.sigma_x, self.sigma_y)\n",
    "        \n",
    "        # This is from: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py\n",
    "        # z = tf.square(tf.div(norm1, s1)) + tf.square(tf.div(norm2, s2))\n",
    "        #     - 2 * tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "         \n",
    "        # below is the deconstruction of the above linez\n",
    "        z_first_term = F.pow(F.div(norm1, self.sigma_x), 2)\n",
    "        z_second_term = F.pow(F.div(norm2, self.sigma_y), 2)\n",
    "        z_last_term_inner = F.mul(self.rho_xy, F.mul(norm1, norm2))\n",
    "        z_last_term_middle = F.div(z_last_term_inner, s1s2)\n",
    "        tmp_z = xp.ones(z_last_term_middle.shape) * -2\n",
    "        z_last_term = F.mul(tmp_z, z_last_term_middle)\n",
    "        z = F.add(F.add(z_first_term, z_second_term), z_last_term)\n",
    "        negRho = F.sub(xp.ones(self.rho_xy.shape), F.pow(self.rho_xy, 2))\n",
    "\n",
    "        \n",
    "        result = F.exp(F.div(-z, 2 * negRho))\n",
    "        deno_first_term = xp.ones(self.sigma_x.shape) * 2 * math.pi\n",
    "        denom_second_term = F.mul(s1s2, F.pow(negRho, 0.5))\n",
    "        denom = F.mul(deno_first_term, denom_second_term)\n",
    "        result = F.div(result, denom)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # x1_data and x2_data are the real x and y coordinates of the stroke\n",
    "    def get_lossfunc(self, x_delta, y_delta, mask):\n",
    "        result0 = self.gaussian_pdf(x_delta, y_delta)\n",
    "        # check if result0 has inf or nan\n",
    "        # result0 shape is (batch_size, seq_len, n_distributions) 3D\n",
    "        result1 = F.mul(result0, self.pi_logits) # pi_logits shape is (batch_size, seq_len, n_distributions)\n",
    "        \n",
    "        result1 = F.sum(result1, axis=2, keepdims=True) # sum over the distributions\n",
    "        # the result1 shape is (batch_size, seq_len, 1)\n",
    "        # we reshape it to (batch_size, seq_len)\n",
    "        result1 = F.reshape(result1, result1.shape[:-1]) # result.shape[:-1] is (batch_size, seq_len)\n",
    "        \n",
    "        dtype = result1.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        result1 = F.clip(result1, 1e-5, max_dtype) \n",
    "\n",
    "        result1 = -F.log(result1) # result1 shape is (batch_size, seq_len)\n",
    "        # mask shape is also (batch_size, seq_len)\n",
    "        result1 = F.mul(result1, mask) # we multiply the mask to ignore the padding\n",
    "        \n",
    "        \n",
    "        # make the value to be one number\n",
    "        \n",
    "        \n",
    "        return F.mean(result1)\n",
    "    \n",
    "    def get_pi_idx(self, x, out_pi_elem):\n",
    "        # pdf shape is (batch_size, seq_len, n_distributions)\n",
    "        # let us only get the first batch\n",
    "        pdf = out_pi_elem\n",
    "        N = pdf.size\n",
    "        accumulate = 0\n",
    "        # print(pdf.size, 'pdf size')\n",
    "        # print(F.sum(pdf), 'sum of pdf')\n",
    "        # print(out_pi_elem.shape, 'out_pi_elem shape')\n",
    "        # print(x, 'x in pi idx', pdf, \" pdf\", pdf.shape)\n",
    "        # print(\"hello\",pdf[0], x)\n",
    "        for i in range(0, N):\n",
    "            # print(pdf[i].data, 'pdf[i].data')\n",
    "            accumulate += pdf[i].data\n",
    "            if accumulate >= x:\n",
    "                return i\n",
    "            # print(accumulate, 'accumulate')\n",
    "        print('error with sampling ensemble')\n",
    "        return -1\n",
    "    \n",
    "    # M means the number of samples\n",
    "    def sample(self, count, M=15):\n",
    "        xp = cuda.get_array_module(self.pi_logits)\n",
    "        # get the index of the distribution\n",
    "        \n",
    "        result = xp.random.rand(count, M, 3) # initially random [0,1]\n",
    "        # print(result.shape, 'result shape')\n",
    "        # we will get result for delta_x and delta_y\n",
    "        rn = xp.random.rand(count, M, 2) \n",
    "        mu = 0\n",
    "        std = 0\n",
    "        idx = 0\n",
    "        \n",
    "        # currently the pi logits shape is (batch_size, seq_len, n_distributions)\n",
    "        # we will only get the first batch for now'\n",
    "        # print(self.pi_logits.shape, 'pi logits shape')\n",
    "        # print(self.pi_logits[0].shape, 'pi logits shape')\n",
    "        # print(\"get sum of pi\", F.sum(self.pi_logits[0]))\n",
    "        out_pi = self.pi_probs[0] # out_pi shape is (seq_len, n_distributions)\n",
    "        # print(out_pi.shape, 'out_pi shape')\n",
    "        # print(\"mu_x\", self.mu_x.shape)\n",
    "        # print(\"std\", self.sigma_x.shape)\n",
    "        \n",
    "        # we do not need to get batch size since we are only getting the first batch\n",
    "        mu_x = self.mu_x[0]\n",
    "        mu_y = self.mu_y[0]\n",
    "        sigma_x = self.sigma_x[0]\n",
    "        sigma_y = self.sigma_y[0]\n",
    "        rho_xy = self.rho_xy[0]\n",
    "        \n",
    "        for j in range(M):\n",
    "            for i in range(count):\n",
    "                # we only get the first element since we only need one\n",
    "                idx = self.get_pi_idx(result[i, j, 0], out_pi[i])\n",
    "                mu = [mu_x[i, idx], mu_y[i, idx]]\n",
    "                std = [sigma_x[i, idx], sigma_y[i, idx]]\n",
    "                rho = rho_xy[i, idx]\n",
    "                \n",
    "            \n",
    "                # print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\n",
    "                result_x_y = (mu + rn[i, j] * std)\n",
    "                # print(result_x_y[0].data, 'this is resuult')\n",
    "                result[i, j, 0] = result_x_y[0].data\n",
    "                result[i, j, 1] = result_x_y[1].data\n",
    "        return result\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, d_z, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=5, hidden_size=hidden_size)\n",
    "        self.mu_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "        self.sigma_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_z = d_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden = self.lstm(x)\n",
    "\n",
    "        # hidden = hidden[:,-1,:]        \n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        h, c = None, None\n",
    "        for i in range(seq_len):\n",
    "            if h is None or c is None:\n",
    "                h, c = self.lstm(x[:, i, :])\n",
    "            else:\n",
    "                h, c = self.lstm(x[:, i, :], h, c)\n",
    "\n",
    "        mu = self.mu_head(h)\n",
    "        sigma_hat = self.sigma_head(h)\n",
    "        sigma = F.exp(sigma_hat / 2.)\n",
    "\n",
    "        xp = cuda.get_array_module(mu)\n",
    "        z = mu + sigma * xp.random.normal(0, 1, mu.shape)\n",
    "        return z, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, d_z, hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=d_z+5, hidden_size=hidden_size)\n",
    "\n",
    "        self.init_h = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "        self.init_c = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "\n",
    "        self.pi_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.rho_xy_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "\n",
    "        self.q_head = L.Linear(in_size=hidden_size, out_size=3)\n",
    "\n",
    "        self.n_distributions = n_distributions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, z, h=None, c=None):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        h, c = None, None\n",
    "        if h is None and c is None:\n",
    "            h = F.tanh(self.init_h(z))\n",
    "            c = F.tanh(self.init_c(z))\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(seq_len):\n",
    "            h, c = self.lstm(x[:, i, :], h, c)\n",
    "            if outputs == None:\n",
    "                outputs = F.expand_dims(h, 1)\n",
    "            else:\n",
    "                outputs = F.cat((outputs, F.expand_dims(h, 1)), axis=1)\n",
    "\n",
    "        # hidden Needs to chagned to output of lstm\n",
    "        # print(outputs.shape)\n",
    "        # print(self.q_head(outputs).shape)\n",
    "\n",
    "        outputs= F.reshape(outputs, (-1, self.hidden_size))\n",
    "        q_logits = F.log_softmax(self.q_head(outputs))\n",
    "        # print(q_logits.shape, \"q_logits\")\n",
    "\n",
    "        pi_logits = self.pi_head(outputs)\n",
    "        mu_x = self.mu_x_head(outputs)\n",
    "        mu_y = self.mu_y_head(outputs)\n",
    "        sigma_x = self.sigma_x_head(outputs)\n",
    "        sigma_y = self.sigma_y_head(outputs)\n",
    "        rho_xy = self.rho_xy_head(outputs)\n",
    "\n",
    "        pi_logits = F.reshape(pi_logits, (-1, seq_len, self.n_distributions))\n",
    "        mu_x = F.reshape(mu_x, (-1, seq_len, self.n_distributions))\n",
    "        mu_y = F.reshape(mu_y, (-1, seq_len, self.n_distributions))\n",
    "        sigma_x = F.reshape(sigma_x, (-1, seq_len, self.n_distributions))\n",
    "        sigma_y = F.reshape(sigma_y, (-1, seq_len, self.n_distributions))\n",
    "        rho_xy = F.reshape(rho_xy, (-1, seq_len, self.n_distributions))\n",
    "        \n",
    "        q_logits = F.reshape(q_logits, (-1, seq_len, 3))\n",
    "\n",
    "\n",
    "        bgm = BivariateGaussianMixture(pi_logits, mu_x, mu_y, F.exp(sigma_x), F.exp(sigma_y), F.tanh(rho_xy))\n",
    "        return bgm, q_logits, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReconstructionLoss(target, mask, bgm, q_logits):\n",
    "#     xp = cuda.get_array_module(mask)\n",
    "#     xy = target[:, :, 0:2]\n",
    "#     xy = xy[:, :, xp.newaxis, :]\n",
    "#     xy = xp.tile(xy, (1, 1, bgm.n_distributions, 1))\n",
    "    \n",
    "#     # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "    \n",
    "#     x = xy[:,:,:,0]\n",
    "#     y = xy[:,:,:,0]\n",
    "    \n",
    "#     loss_stroke = F.mul(bgm.get_lossfunc(x, y), mask)\n",
    "    \n",
    "#     loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "    \n",
    "#     return F.add(loss_stroke, loss_pen)\n",
    "\n",
    "\n",
    "def ReconstructionLoss(mask, target, bgm, q_logits):\n",
    "        xp = cuda.get_array_module(mask)\n",
    "        # target is a 3 dimensional array\n",
    "        # xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        xy = target[:, :, 0:2]\n",
    "        x = xy[:, :, 0]\n",
    "        y = xy[:, :, 1]\n",
    "        \n",
    "        distributions = bgm.n_distributions\n",
    "        stacked_x = None\n",
    "        stacked_y = None\n",
    "        for i in range(distributions):\n",
    "            if stacked_x is None:\n",
    "                stacked_x = F.expand_dims(x, axis=2)\n",
    "                stacked_y = F.expand_dims(y, axis=2)\n",
    "            else:\n",
    "                stacked_x = F.cat((stacked_x, F.expand_dims(x, axis=2)), axis=2)\n",
    "                stacked_y = F.cat((stacked_y, F.expand_dims(y, axis=2)), axis=2)\n",
    "        \n",
    "        # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "        \n",
    "        \n",
    "        # x = xp.tile(xy, expanded_shape)\n",
    "        # y = xp.tile(xy, expanded_shape)\n",
    "        # loss_stroke 에 문제\n",
    "        \n",
    "        loss_stroke = bgm.get_lossfunc(stacked_x, stacked_y, mask)\n",
    "        \n",
    "        loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "        \n",
    "        return F.add(loss_stroke, loss_pen)\n",
    "        \n",
    "        \n",
    "\n",
    "def KLDivergenceLoss(mu, sigma):\n",
    "    xp = cuda.get_array_module(mu)\n",
    "    tmp = xp.ones(sigma.shape)\n",
    "    inner_1 = F.add(tmp, sigma)\n",
    "    inner_2 = F.add(F.pow(mu, 2), F.exp(sigma))\n",
    "    inner = F.sub(inner_1, inner_2)\n",
    "    tmp2 = xp.ones(inner.shape) * -2\n",
    "    return F.mean(F.div(inner, tmp2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, d_z, enc_hidden_size, dec_hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_z, enc_hidden_size)\n",
    "        self.decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z, mu, sigma = self.encoder(x)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        xp = cuda.get_array_module(z)\n",
    "\n",
    "        expanded_z = F.expand_dims(z, axis=1)\n",
    "        \n",
    "        z_stack = None\n",
    "        \n",
    "        for i in range(seq_len - 1):\n",
    "            if i == 0:\n",
    "                z_stack = expanded_z\n",
    "            else:\n",
    "                z_stack = F.cat((z_stack, expanded_z), axis=1)\n",
    "        # x = F.expand_dims(z, axis=1)\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits, _, _ = self.decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "\n",
    "\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        # \n",
    "        return kl_loss + rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = 4\n",
    "enc_hidden_size = 32\n",
    "dec_hidden_size = 64\n",
    "n_distributions = 8\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.optimizers import Adam\n",
    "\n",
    "data = np.load('./data/sketchrnn_apple.npz', encoding='latin1', allow_pickle=True)\n",
    "\n",
    "strokes = StrokesDataset(data['train'], batch_size=batch_size, max_seq_length=200, gpu=False, shuffle=False)\n",
    "\n",
    "encoder = Encoder(d_z, enc_hidden_size)\n",
    "decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "encoder_optimizer = Adam().setup(encoder)\n",
    "decoder_optimizer = Adam().setup(decoder)\n",
    "\n",
    "if use_gpu:\n",
    "    encoder.to_gpu()\n",
    "    decoder.to_gpu()\n",
    "    strokes.to_gpu()\n",
    "    xp = dezero.cuda.cupy\n",
    "else:\n",
    "    xp = np\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, cnt_loss: 3.6689511996561004, loss.data: 3.6689511996561004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m encoder\u001b[38;5;241m.\u001b[39mcleargrads()\n\u001b[1;32m     33\u001b[0m decoder\u001b[38;5;241m.\u001b[39mcleargrads()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     36\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:130\u001b[0m, in \u001b[0;36mVariable.backward\u001b[0;34m(self, retain_grad, create_graph)\u001b[0m\n\u001b[1;32m    127\u001b[0m gys \u001b[38;5;241m=\u001b[39m [output()\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39moutputs]  \u001b[38;5;66;03m# output is weakref\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m using_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_backprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, create_graph):\n\u001b[0;32m--> 130\u001b[0m     gxs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gxs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    132\u001b[0m         gxs \u001b[38;5;241m=\u001b[39m (gxs,)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:385\u001b[0m, in \u001b[0;36mLinear.backward\u001b[0;34m(self, gy)\u001b[0m\n\u001b[1;32m    383\u001b[0m gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m b\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sum_to(gy, b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    384\u001b[0m gx \u001b[38;5;241m=\u001b[39m matmul(gy, W\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m--> 385\u001b[0m gW \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gx, gW, gb\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:371\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(x, W)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul\u001b[39m(x, W):\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [as_variable(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    206\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 207\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    209\u001b[0m     ys \u001b[38;5;241m=\u001b[39m (ys,)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:360\u001b[0m, in \u001b[0;36mMatMul.forward\u001b[0;34m(self, x, W)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, W):\n\u001b[0;32m--> 360\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    cnt = 0\n",
    "    cnt_loss = 0\n",
    "    for x, t in strokes:\n",
    "        encoder.lstm.reset_state()\n",
    "        decoder.lstm.reset_state()\n",
    "        z, mu, sigma = encoder(x)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        xp = cuda.get_array_module(z)\n",
    "\n",
    "        expanded_z = F.expand_dims(z, axis=1)\n",
    "        \n",
    "        z_stack = None\n",
    "        \n",
    "        for i in range(seq_len - 1):\n",
    "            if i == 0:\n",
    "                z_stack = expanded_z\n",
    "            else:\n",
    "                z_stack = F.cat((z_stack, expanded_z), axis=1)\n",
    "        # x = F.expand_dims(z, axis=1)\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits, _, _ = decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        loss = kl_loss + rec_loss\n",
    "\n",
    "        encoder.cleargrads()\n",
    "        decoder.cleargrads()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.update()\n",
    "        decoder_optimizer.update()\n",
    "        \n",
    "        cnt_loss += loss.data\n",
    "        cnt+=1\n",
    "        print(f\"{cnt}, cnt_loss: {cnt_loss}, loss.data: {loss.data}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch}: {cnt_loss/cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z with additional axis (1, 1000, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(seq)\n\u001b[0;32m---> 67\u001b[0m \u001b[43mconditional_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mconditional_generation\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(longest_seq_len):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# we must make s a batch (1, 1, 5)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     morphed_s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mreshape(seq, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 33\u001b[0m     dec_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmorphed_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dec_inputs\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdec input shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     bgm, q_logits, h, c \u001b[38;5;241m=\u001b[39m decoder(dec_inputs, z, h, c)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:137\u001b[0m, in \u001b[0;36mcat\u001b[0;34m(inputs, axis)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcat\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCat\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [as_variable(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    206\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 207\u001b[0m ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ys, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    209\u001b[0m     ys \u001b[38;5;241m=\u001b[39m (ys,)\n",
      "File \u001b[0;32m~/Github/Sketch-RNN/dezero/functions.py:115\u001b[0m, in \u001b[0;36mCat.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m    114\u001b[0m     xp \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mget_array_module(inputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 115\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 1000"
     ]
    }
   ],
   "source": [
    "\n",
    "# conditional generation is on one sequence\n",
    "def conditional_generation(inputs):\n",
    "    \n",
    "    longest_seq_len = inputs.shape[1]\n",
    "\n",
    "    \n",
    "    xp = cuda.get_array_module(inputs)\n",
    "    \n",
    "    z, _, _ = encoder(inputs)\n",
    "    \n",
    "    # z_shape is (batch_size, d_z) = (1, d_z)\n",
    "    seq_len = inputs.shape[1]\n",
    "    seq_x = []\n",
    "    seq_y = []\n",
    "    seq_pen = []\n",
    "    \n",
    "    sos = xp.zeros(5)\n",
    "    sos[2] = 1\n",
    "    s = sos\n",
    "    \n",
    "    seq = xp.array([s])\n",
    "    \n",
    "    h = None\n",
    "    c = None\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"z with additional axis\", F.expand_dims(z, axis=0).shape)\n",
    "    with dezero.no_grad():\n",
    "        for i in range(longest_seq_len):\n",
    "            # we must make s a batch (1, 1, 5)\n",
    "            morphed_s = F.reshape(seq, (1, 1, -1))\n",
    "            dec_inputs = F.cat((morphed_s, F.expand_dims(z, axis=0)), axis=2)\n",
    "            \n",
    "            print(dec_inputs.shape, \"dec input shape\")\n",
    "            \n",
    "\n",
    "            \n",
    "            bgm, q_logits, h, c = decoder(dec_inputs, z, h, c)\n",
    "            \n",
    "            # sample\n",
    "            \n",
    "            s = bgm.sample(1) # x, y\n",
    "            \n",
    "            q_props = F.softmax(q_logits) # q_logits size is (batch_size, seq_len, 3)\n",
    "            \n",
    "            q_choice = xp.random.choice(3, 1, p=q_props[0, 0].data)\n",
    "            \n",
    "            # size of q_choice is (1,)\n",
    "            \n",
    "            # we must make s a batch (1, 1, 5)\n",
    "            \n",
    "            \n",
    "            \n",
    "            s = F.cat((s, q_choice))\n",
    "            \n",
    "            \n",
    "            print(s)\n",
    "            \n",
    "            \n",
    "            seq.append(s)\n",
    "            if s[4] == 1:\n",
    "                break\n",
    "    \n",
    "    print(seq)\n",
    "\n",
    "conditional_generation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def sample(self, x, temperature=1.0):\n",
    "        # x is a batch of stroke data\n",
    "        longest_seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        z, _, _ = self.encoder(x)\n",
    "        xp = cuda.get_array_module(x)\n",
    "        \n",
    "        s = xp.zeros(5)\n",
    "        # mark the start of the sequence\n",
    "        s[2] = 1\n",
    "        \n",
    "        print(s)\n",
    "        # s is the initial stroke\n",
    "        # we are going to create a sequence of strokes\n",
    "        # we first \"\"\n",
    "        seq = xp.array([s])\n",
    "        \n",
    "        h = None\n",
    "        c = None\n",
    "        \n",
    "        \n",
    "        with dezero.no_grad():\n",
    "            for i in range(longest_seq_len):\n",
    "                expanded_z = F.expand_dims(z, axis=1)\n",
    "                # change s to a 3D array to have (1,1,)\n",
    "                \n",
    "                morphed_s = F.reshape(s, (1, 1, -1))\n",
    "                \n",
    "                stacked_s = None\n",
    "                for i in range(batch_size):\n",
    "                    if stacked_s is None:\n",
    "                        stacked_s = morphed_s\n",
    "                    else:\n",
    "                        stacked_s = F.cat((stacked_s, morphed_s), axis=0)\n",
    "                        \n",
    "                \n",
    "                # print(stacked_s.shape, expanded_z.shape, \"stacked_s, expanded_z\")\n",
    "                inputs = F.cat((stacked_s, expanded_z), axis=2)\n",
    "                # 여기 아래 부분에서 오류가 생긴다\n",
    "                \n",
    "                if h is None and c is None:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z)\n",
    "                else:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z, h, c)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                s = self._sample_step(bgm, q_logits, temperature)\n",
    "                \n",
    "                xp.append(seq, s)\n",
    "                # seq.append(s)\n",
    "                \n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "                \n",
    "        seq = F.cat(seq, axis=1)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "                \n",
    "    def _sample_step(self, bgm, q_logits, temperature):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        bgm.set_temperature(temperature)\n",
    "        \n",
    "        # pring if bgm pi sum is 1\n",
    "        \n",
    "        # print(bgm.pi_logits.shape, 'pi logits shape in sample step')\n",
    "        seq_len = bgm.pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(bgm.pi_logits, axis=2), (-1, seq_len)), 'sum of pi _sample_step')\n",
    "        gen_result = bgm.sample(1)\n",
    "        \n",
    "        # we need categorical distribution q for pen state\n",
    "        # convert scaled logits to probabilities\n",
    "        probabilities = F.softmax(q_logits / temperature, axis=-1)\n",
    "        \n",
    "        gen_x = gen_result[:, :, 0]\n",
    "        gen_y = gen_result[:, :, 1]\n",
    "        \n",
    "        # sample from categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        xp = cuda.get_array_module(gen_x)\n",
    "        stroke = xp.zeros(5, dtype=q_logits.dtype)\n",
    "        \n",
    "        \n",
    "        print(gen_x.shape, gen_y.shape, probabilities, \"gen_x, gen_y, probabilities\")\n",
    "        # print(stroke, \"stroke\")\n",
    "        # fill in gen_x and gen_y\n",
    "        # print(gen_x[0][0], gen_y[0][0])\n",
    "        stroke[0] = gen_x[0][0]\n",
    "        stroke[1] = gen_y[0][0]\n",
    "        \n",
    "        # sample from the categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        print(probabilities[0][0].data, 'probabilities')\n",
    "        # choose pen state from 0, 1, 2\n",
    "        pen = xp.random.choice(3, 1, p=probabilities[0][0].data)\n",
    "        print(pen)\n",
    "        \n",
    "        stroke[2 + pen] = 1\n",
    "        \n",
    "        return stroke\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (1000, 64, 5)\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'multiply'>, '__call__', array([0.03604792, 0.736856  ]), [variable(0.44269818748653245), variable(0.6282123466995799)]): 'ndarray', 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# add dimension to make it compatible with the model, batch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minput shape: \u001b[39m\u001b[39m\"\u001b[39m,batch_data\u001b[39m.\u001b[39mshape) \u001b[39m# we have one batch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sample \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(batch_data, temperature\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m)\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     bgm, q_logits, h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(inputs, z, h, c)\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_step(bgm, q_logits, temperature)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m xp\u001b[39m.\u001b[39mappend(seq, s)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# seq.append(s)\u001b[39;00m\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m seq_len \u001b[39m=\u001b[39m bgm\u001b[39m.\u001b[39mpi_logits\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# print(F.reshape(F.sum(bgm.pi_logits, axis=2), (-1, seq_len)), 'sum of pi _sample_step')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m gen_result \u001b[39m=\u001b[39m bgm\u001b[39m.\u001b[39;49msample(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# we need categorical distribution q for pen state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# convert scaled logits to probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m probabilities \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(q_logits \u001b[39m/\u001b[39m temperature, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m rho \u001b[39m=\u001b[39m rho_xy[i, idx]\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m# print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m result_x_y \u001b[39m=\u001b[39m (mu \u001b[39m+\u001b[39m rn[i, j] \u001b[39m*\u001b[39;49m std)\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m# print(result_x_y[0].data, 'this is resuult')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m result[i, j, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m result_x_y[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1285\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__mul__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'multiply'>, '__call__', array([0.03604792, 0.736856  ]), [variable(0.44269818748653245), variable(0.6282123466995799)]): 'ndarray', 'list'"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(encoder, decoder)\n",
    "\n",
    "batch_data = x[0:batch_size] # since we trained with batch size of 4, we can only sample 4 sketches\n",
    "# add dimension to make it compatible with the model, batch\n",
    "print(\"input shape: \",batch_data.shape) # we have one batch\n",
    "\n",
    "sample = sampler.sample(batch_data, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
