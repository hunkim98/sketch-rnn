{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         2.4.1\n",
      "comm              0.2.2\n",
      "contourpy         1.2.1\n",
      "cupy-cuda11x      13.0.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.1\n",
      "decorator         5.1.1\n",
      "exceptiongroup    1.2.0\n",
      "executing         2.0.1\n",
      "fastrlock         0.8.2\n",
      "fonttools         4.50.0\n",
      "ipykernel         6.29.4\n",
      "ipython           8.23.0\n",
      "jedi              0.19.1\n",
      "jupyter_client    8.6.1\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.5\n",
      "matplotlib        3.8.3\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.6.0\n",
      "numpy             1.23.0\n",
      "packaging         24.0\n",
      "parso             0.8.3\n",
      "pexpect           4.9.0\n",
      "pillow            10.3.0\n",
      "pip               23.0.1\n",
      "platformdirs      4.2.0\n",
      "prompt-toolkit    3.0.43\n",
      "psutil            5.9.8\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "Pygments          2.17.2\n",
      "pyparsing         3.1.2\n",
      "python-dateutil   2.9.0.post0\n",
      "pyzmq             25.1.2\n",
      "scipy             1.13.0\n",
      "setuptools        65.5.1\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tornado           6.4\n",
      "traitlets         5.14.2\n",
      "typing_extensions 4.10.0\n",
      "wcwidth           0.2.13\n",
      "wheel             0.41.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy==1.23.0 \n",
    "# !pip install -U cupy-cuda11x\n",
    "# !pip install -U cupy-cuda12x\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "import dezero.models as M\n",
    "from dezero import cuda\n",
    "from dezero.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from dezero import cuda\n",
    "use_gpu = False\n",
    "use_gpu = cuda.gpu_enable\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "x = cp.arange(6).reshape(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import cuda\n",
    "from typing import List, Optional, Tuple, Any\n",
    "import math\n",
    "\n",
    "class StrokesDataset(dezero.DataLoader):\n",
    "    def __init__(self, data, batch_size, max_seq_length: int, scale: Optional[float] = None, shuffle=True, gpu=False):\n",
    "        stroke_data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(data)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        \n",
    "        for seq in data:\n",
    "            # we will deem a sequence that is less than 10 as too short and thus ignore it\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # clamp the delta x and delta y to [-1000, 1000]\n",
    "                seq = xp.minimum(seq, 1000)\n",
    "                seq = xp.maximum(seq, -1000)\n",
    "                \n",
    "                seq = xp.array(seq, dtype=xp.float32)\n",
    "                stroke_data.append(seq)\n",
    "        \n",
    "        if scale is None:\n",
    "            # calculate the scale factor\n",
    "            # the scale factor is the standard deviation of the x and y coordinates\n",
    "            # mean is not adjusted for simplicity\n",
    "            # 0:2 means the first two columns of the array which are x and y coordinates\n",
    "            scale = xp.std(xp.concatenate([xp.ravel(s[:,0:2]) for s in stroke_data]))\n",
    "        \n",
    "        longest_seq_len = max([len(seq) for seq in stroke_data])\n",
    "        \n",
    "        # we add two extra columns to the dataset since we currently there are only 3 columns in the dataset\n",
    "        # additional two columns are for changing the last point 1/0 to a one-hot vector\n",
    "        temp_stroke_dataset = xp.zeros((len(stroke_data), longest_seq_len + 2, 5), dtype=xp.float32)\n",
    "        \n",
    "        # self.mask is used to mark areas of the sequence that are not used\n",
    "        # we first initialize it to zero\n",
    "        temp_mask_dataset = xp.zeros((len(stroke_data), longest_seq_len + 1))\n",
    "        \n",
    "        self.dataset = []\n",
    "        \n",
    "        # start of sequence is [0, 0, 1, 0, 0]\n",
    "        \n",
    "        for i, seq in enumerate(stroke_data):\n",
    "            seq = xp.array(seq, dtype=xp.float32)\n",
    "            len_seq = len(seq)\n",
    "            \n",
    "            # we start from 1 to leave the first row for the start of sequence token\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 0:2] = seq[:, :2] / scale # this is the x and y coordinates\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 2] = 1 - seq[:, 2] # this is the pen down\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 3] = seq[:, 2] # this is the pen up\n",
    "            temp_stroke_dataset[i, len_seq + 1, 4] = 1  # this is the end of sequence token\n",
    "            temp_mask_dataset[i, :len_seq + 1] = 1 # mask is on until the end of the sequence \n",
    "            # self.mask is used to mark areas of the sequence that are not used\n",
    "            # for example, if the sequence is shorter than the longest sequence, we use mask to ignore the rest of the sequence\n",
    "            # an example of mask is [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        temp_stroke_dataset[:, 0, 2] = 1\n",
    "        \n",
    "        for i in range(len(stroke_data)):\n",
    "            self.dataset.append([temp_stroke_dataset[i], temp_mask_dataset[i]])\n",
    "        \n",
    "        \n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "\n",
    "# According to other estimates\n",
    "# the number of distributions in the mixture model is 20\n",
    "# https://github.com/Shun14/sketch-rnn-kanji\n",
    "# https://nn.labml.ai/sketch_rnn/index.html\n",
    "\n",
    "# This is for getting the loss of delta_x and delta_y\n",
    "class BivariateGaussianMixture:\n",
    "    def __init__(self, pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
    "        # check if the pi_probs sum for each sequence is 1\n",
    "        # print('test pi', test_pi.shape) # pi shape is (batch_size, seq_len, n_distributions)\n",
    "        # check if the pi probabilities sum to 1\n",
    "        # seq_len = pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(pi_logits, axis=2), (-1, seq_len)), 'sum of pi')\n",
    "        self.pi_logits = pi_logits\n",
    "        self.pi_probs = F.softmax(pi_logits, axis=2)\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "\n",
    "        xp = cuda.cupy if cuda.gpu_enable else np\n",
    "    \n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        return self.pi_logits.shape[-1]\n",
    "    \n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.pi_logits /= temperature\n",
    "        self.pi_probs = F.softmax(self.pi_logits, axis=2) # we do this to make sure the pi probabilities sum to 1\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "    \n",
    "    def gaussian_pdf(self, x_delta, y_delta):\n",
    "        # the result means the probability of y in the normal distribution\n",
    "        # we check the probability of y in the normal distribution\n",
    "        # if the probability is high, the result is close to 1\n",
    "        # x_delta and y_delta shape are (batch_size, seq_len, hidden_size)\n",
    "        # mu_x and mu_y shape are (batch_size, seq_len, n_distributions)\n",
    "        norm1 = F.sub(x_delta, self.mu_x)\n",
    "        norm2 = F.sub(y_delta, self.mu_y)\n",
    "        xp = cuda.get_array_module(norm1)\n",
    "\n",
    "        dtype = self.sigma_x.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        self.sigma_x = F.clip(self.sigma_x, 1e-5, max_dtype)\n",
    "        self.sigma_y = F.clip(self.sigma_y, 1e-5, max_dtype)\n",
    "        self.rho_xy = F.clip(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "        \n",
    "        s1s2 = F.mul(self.sigma_x, self.sigma_y)\n",
    "        \n",
    "        # This is from: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py\n",
    "        # z = tf.square(tf.div(norm1, s1)) + tf.square(tf.div(norm2, s2))\n",
    "        #     - 2 * tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "         \n",
    "        # below is the deconstruction of the above linez\n",
    "        z_first_term = F.pow(F.div(norm1, self.sigma_x), 2)\n",
    "        z_second_term = F.pow(F.div(norm2, self.sigma_y), 2)\n",
    "        z_last_term_inner = F.mul(self.rho_xy, F.mul(norm1, norm2))\n",
    "        z_last_term_middle = F.div(z_last_term_inner, s1s2)\n",
    "        tmp_z = xp.ones(z_last_term_middle.shape) * -2\n",
    "        z_last_term = F.mul(tmp_z, z_last_term_middle)\n",
    "        z = F.add(F.add(z_first_term, z_second_term), z_last_term)\n",
    "        negRho = F.sub(xp.ones(self.rho_xy.shape), F.pow(self.rho_xy, 2))\n",
    "\n",
    "        \n",
    "        result = F.exp(F.div(-z, 2 * negRho))\n",
    "        deno_first_term = xp.ones(self.sigma_x.shape) * 2 * math.pi\n",
    "        denom_second_term = F.mul(s1s2, F.pow(negRho, 0.5))\n",
    "        denom = F.mul(deno_first_term, denom_second_term)\n",
    "        result = F.div(result, denom)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # x1_data and x2_data are the real x and y coordinates of the stroke\n",
    "    def get_lossfunc(self, x_delta, y_delta, mask):\n",
    "        result0 = self.gaussian_pdf(x_delta, y_delta)\n",
    "        # check if result0 has inf or nan\n",
    "        # result0 shape is (batch_size, seq_len, n_distributions) 3D\n",
    "        result1 = F.mul(result0, self.pi_logits) # pi_logits shape is (batch_size, seq_len, n_distributions)\n",
    "        \n",
    "        result1 = F.sum(result1, axis=2, keepdims=True) # sum over the distributions\n",
    "        # the result1 shape is (batch_size, seq_len, 1)\n",
    "        # we reshape it to (batch_size, seq_len)\n",
    "        result1 = F.reshape(result1, result1.shape[:-1]) # result.shape[:-1] is (batch_size, seq_len)\n",
    "        \n",
    "        dtype = result1.dtype\n",
    "        max_dtype = xp.finfo(dtype).max\n",
    "        result1 = F.clip(result1, 1e-5, max_dtype) \n",
    "\n",
    "        result1 = -F.log(result1) # result1 shape is (batch_size, seq_len)\n",
    "        # mask shape is also (batch_size, seq_len)\n",
    "        result1 = F.mul(result1, mask) # we multiply the mask to ignore the padding\n",
    "        \n",
    "        \n",
    "        # make the value to be one number\n",
    "        \n",
    "        \n",
    "        return F.mean(result1)\n",
    "    \n",
    "    def get_pi_idx(self, x, out_pi_elem):\n",
    "        # pdf shape is (batch_size, seq_len, n_distributions)\n",
    "        # let us only get the first batch\n",
    "        pdf = out_pi_elem\n",
    "        N = pdf.size\n",
    "        accumulate = 0\n",
    "        # print(pdf.size, 'pdf size')\n",
    "        # print(F.sum(pdf), 'sum of pdf')\n",
    "        # print(out_pi_elem.shape, 'out_pi_elem shape')\n",
    "        # print(x, 'x in pi idx', pdf, \" pdf\", pdf.shape)\n",
    "        # print(\"hello\",pdf[0], x)\n",
    "        for i in range(0, N):\n",
    "            # print(pdf[i].data, 'pdf[i].data')\n",
    "            accumulate += pdf[i].data\n",
    "            if accumulate >= x:\n",
    "                return i\n",
    "            # print(accumulate, 'accumulate')\n",
    "        print('error with sampling ensemble')\n",
    "        return -1\n",
    "    \n",
    "    # M means the number of samples\n",
    "    def sample(self, count, M=15):\n",
    "        xp = cuda.get_array_module(self.pi_logits)\n",
    "        # get the index of the distribution\n",
    "        \n",
    "        result = xp.random.rand(count, M, 3) # initially random [0,1]\n",
    "        # print(result.shape, 'result shape')\n",
    "        # we will get result for delta_x and delta_y\n",
    "        rn = xp.random.rand(count, M, 2) \n",
    "        mu = 0\n",
    "        std = 0\n",
    "        idx = 0\n",
    "        \n",
    "        # currently the pi logits shape is (batch_size, seq_len, n_distributions)\n",
    "        # we will only get the first batch for now'\n",
    "        # print(self.pi_logits.shape, 'pi logits shape')\n",
    "        # print(self.pi_logits[0].shape, 'pi logits shape')\n",
    "        # print(\"get sum of pi\", F.sum(self.pi_logits[0]))\n",
    "        out_pi = self.pi_probs[0] # out_pi shape is (seq_len, n_distributions)\n",
    "        # print(out_pi.shape, 'out_pi shape')\n",
    "        # print(\"mu_x\", self.mu_x.shape)\n",
    "        # print(\"std\", self.sigma_x.shape)\n",
    "        \n",
    "        # we do not need to get batch size since we are only getting the first batch\n",
    "        mu_x = self.mu_x[0]\n",
    "        mu_y = self.mu_y[0]\n",
    "        sigma_x = self.sigma_x[0]\n",
    "        sigma_y = self.sigma_y[0]\n",
    "        rho_xy = self.rho_xy[0]\n",
    "        \n",
    "        for j in range(M):\n",
    "            for i in range(count):\n",
    "                # we only get the first element since we only need one\n",
    "                idx = self.get_pi_idx(result[i, j, 0], out_pi[i])\n",
    "                mu = [mu_x[i, idx], mu_y[i, idx]]\n",
    "                std = [sigma_x[i, idx], sigma_y[i, idx]]\n",
    "                rho = rho_xy[i, idx]\n",
    "                \n",
    "            \n",
    "                # print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\n",
    "                result_x_y = (mu + rn[i, j] * std)\n",
    "                # print(result_x_y[0].data, 'this is resuult')\n",
    "                result[i, j, 0] = result_x_y[0].data\n",
    "                result[i, j, 1] = result_x_y[1].data\n",
    "        return result\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, d_z, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=5, hidden_size=hidden_size)\n",
    "        self.mu_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "        self.sigma_head = L.Linear(in_size=hidden_size, out_size=d_z)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_z = d_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden = self.lstm(x)\n",
    "\n",
    "        # hidden = hidden[:,-1,:]        \n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        h, c = None, None\n",
    "        for i in range(seq_len):\n",
    "            if h is None or c is None:\n",
    "                h, c = self.lstm(x[:, i, :])\n",
    "            else:\n",
    "                h, c = self.lstm(x[:, i, :], h, c)\n",
    "\n",
    "        mu = self.mu_head(h)\n",
    "        sigma_hat = self.sigma_head(h)\n",
    "        sigma = F.exp(sigma_hat / 2.)\n",
    "\n",
    "        xp = cuda.get_array_module(mu)\n",
    "        z = mu + sigma * xp.random.normal(0, 1, mu.shape)\n",
    "        return z, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, d_z, hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(in_size=d_z+5, hidden_size=hidden_size)\n",
    "\n",
    "        self.init_h = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "        self.init_c = L.Linear(in_size=d_z, out_size=hidden_size)\n",
    "\n",
    "        self.pi_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.mu_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_x_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.sigma_y_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "        self.rho_xy_head = L.Linear(in_size=hidden_size, out_size=n_distributions)\n",
    "\n",
    "        self.q_head = L.Linear(in_size=hidden_size, out_size=3)\n",
    "\n",
    "        self.n_distributions = n_distributions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x, z, h=None, c=None):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        h, c = None, None\n",
    "        if h is None and c is None:\n",
    "            h = F.tanh(self.init_h(z))\n",
    "            c = F.tanh(self.init_c(z))\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(seq_len):\n",
    "            h, c = self.lstm(x[:, i, :], h, c)\n",
    "            if outputs == None:\n",
    "                outputs = F.expand_dims(h, 1)\n",
    "            else:\n",
    "                outputs = F.cat((outputs, F.expand_dims(h, 1)), axis=1)\n",
    "\n",
    "        # hidden Needs to chagned to output of lstm\n",
    "        # print(outputs.shape)\n",
    "        # print(self.q_head(outputs).shape)\n",
    "\n",
    "        outputs= F.reshape(outputs, (-1, self.hidden_size))\n",
    "        q_logits = F.log_softmax(self.q_head(outputs))\n",
    "        # print(q_logits.shape, \"q_logits\")\n",
    "\n",
    "        pi_logits = self.pi_head(outputs)\n",
    "        mu_x = self.mu_x_head(outputs)\n",
    "        mu_y = self.mu_y_head(outputs)\n",
    "        sigma_x = self.sigma_x_head(outputs)\n",
    "        sigma_y = self.sigma_y_head(outputs)\n",
    "        rho_xy = self.rho_xy_head(outputs)\n",
    "\n",
    "        pi_logits = F.reshape(pi_logits, (-1, seq_len, self.n_distributions))\n",
    "        mu_x = F.reshape(mu_x, (-1, seq_len, self.n_distributions))\n",
    "        mu_y = F.reshape(mu_y, (-1, seq_len, self.n_distributions))\n",
    "        sigma_x = F.reshape(sigma_x, (-1, seq_len, self.n_distributions))\n",
    "        sigma_y = F.reshape(sigma_y, (-1, seq_len, self.n_distributions))\n",
    "        rho_xy = F.reshape(rho_xy, (-1, seq_len, self.n_distributions))\n",
    "        \n",
    "        q_logits = F.reshape(q_logits, (-1, seq_len, 3))\n",
    "\n",
    "\n",
    "        bgm = BivariateGaussianMixture(pi_logits, mu_x, mu_y, F.exp(sigma_x), F.exp(sigma_y), F.tanh(rho_xy))\n",
    "        return bgm, q_logits, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReconstructionLoss(target, mask, bgm, q_logits):\n",
    "#     xp = cuda.get_array_module(mask)\n",
    "#     xy = target[:, :, 0:2]\n",
    "#     xy = xy[:, :, xp.newaxis, :]\n",
    "#     xy = xp.tile(xy, (1, 1, bgm.n_distributions, 1))\n",
    "    \n",
    "#     # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "    \n",
    "#     x = xy[:,:,:,0]\n",
    "#     y = xy[:,:,:,0]\n",
    "    \n",
    "#     loss_stroke = F.mul(bgm.get_lossfunc(x, y), mask)\n",
    "    \n",
    "#     loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "    \n",
    "#     return F.add(loss_stroke, loss_pen)\n",
    "\n",
    "\n",
    "def ReconstructionLoss(mask, target, bgm, q_logits):\n",
    "        xp = cuda.get_array_module(mask)\n",
    "        # target is a 3 dimensional array\n",
    "        # xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        xy = target[:, :, 0:2]\n",
    "        x = xy[:, :, 0]\n",
    "        y = xy[:, :, 1]\n",
    "        \n",
    "        distributions = bgm.n_distributions\n",
    "        stacked_x = None\n",
    "        stacked_y = None\n",
    "        for i in range(distributions):\n",
    "            if stacked_x is None:\n",
    "                stacked_x = F.expand_dims(x, axis=2)\n",
    "                stacked_y = F.expand_dims(y, axis=2)\n",
    "            else:\n",
    "                stacked_x = F.cat((stacked_x, F.expand_dims(x, axis=2)), axis=2)\n",
    "                stacked_y = F.cat((stacked_y, F.expand_dims(y, axis=2)), axis=2)\n",
    "        \n",
    "        # expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "        \n",
    "        \n",
    "        # x = xp.tile(xy, expanded_shape)\n",
    "        # y = xp.tile(xy, expanded_shape)\n",
    "        # loss_stroke 에 문제\n",
    "        \n",
    "        loss_stroke = bgm.get_lossfunc(stacked_x, stacked_y, mask)\n",
    "        \n",
    "        loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "        \n",
    "        return F.add(loss_stroke, loss_pen)\n",
    "        \n",
    "        \n",
    "\n",
    "def KLDivergenceLoss(mu, sigma):\n",
    "    xp = cuda.get_array_module(mu)\n",
    "    tmp = xp.ones(sigma.shape)\n",
    "    inner_1 = F.add(tmp, sigma)\n",
    "    inner_2 = F.add(F.pow(mu, 2), F.exp(sigma))\n",
    "    inner = F.sub(inner_1, inner_2)\n",
    "    tmp2 = xp.ones(inner.shape) * -2\n",
    "    return F.mean(F.div(inner, tmp2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, d_z, enc_hidden_size, dec_hidden_size, n_distributions):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_z, enc_hidden_size)\n",
    "        self.decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        z, mu, sigma = self.encoder(x)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        xp = cuda.get_array_module(z)\n",
    "\n",
    "        expanded_z = F.expand_dims(z, axis=1)\n",
    "        \n",
    "        z_stack = None\n",
    "        \n",
    "        for i in range(seq_len - 1):\n",
    "            if i == 0:\n",
    "                z_stack = expanded_z\n",
    "            else:\n",
    "                z_stack = F.cat((z_stack, expanded_z), axis=1)\n",
    "        # x = F.expand_dims(z, axis=1)\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits, _, _ = self.decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "\n",
    "\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        # \n",
    "        return kl_loss + rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = 4\n",
    "enc_hidden_size = 32\n",
    "dec_hidden_size = 64\n",
    "n_distributions = 8\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.optimizers import Adam\n",
    "\n",
    "data = np.load('./data/sketchrnn_apple.npz', encoding='latin1', allow_pickle=True)\n",
    "\n",
    "strokes = StrokesDataset(data['train'], batch_size=batch_size, max_seq_length=200, gpu=False, shuffle=False)\n",
    "\n",
    "encoder = Encoder(d_z, enc_hidden_size)\n",
    "decoder = Decoder(d_z, dec_hidden_size, n_distributions)\n",
    "\n",
    "encoder_optimizer = Adam().setup(encoder)\n",
    "decoder_optimizer = Adam().setup(decoder)\n",
    "\n",
    "if use_gpu:\n",
    "    encoder.to_gpu()\n",
    "    decoder.to_gpu()\n",
    "    strokes.to_gpu()\n",
    "    xp = dezero.cuda.cupy\n",
    "else:\n",
    "    xp = np\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.35959469831409024\n",
      "1: 0.3580524130451223\n",
      "2: 0.35701186892460396\n",
      "3: 0.35569990192319306\n",
      "4: 0.35440089086793247\n",
      "5: 0.3530989279295753\n",
      "6: 0.3518548054300383\n",
      "7: 0.3501677911839445\n",
      "8: 0.3486433357003923\n",
      "9: 0.34678756401865646\n",
      "10: 0.34503382455620524\n",
      "11: 0.3431569294426214\n",
      "12: 0.3413124882866275\n",
      "13: 0.33926336417536757\n",
      "14: 0.33660888976012265\n",
      "15: 0.3339500979522905\n",
      "16: 0.3310069141640255\n",
      "17: 0.32861172923097154\n",
      "18: 0.324990223339869\n",
      "19: 0.3208020394908359\n",
      "20: 0.3169408403440783\n",
      "21: 0.3118095687921124\n",
      "22: 0.3073882139068058\n",
      "23: 0.30017432151480233\n",
      "24: 0.29197144011649484\n",
      "25: 0.28390690239343236\n",
      "26: 0.2751426090063988\n",
      "27: 0.2648075614952461\n",
      "28: 0.2511717315371263\n",
      "29: 0.23754717911650827\n",
      "30: 0.22268573730866695\n",
      "31: 0.20813002219402774\n",
      "32: 0.191474792009488\n",
      "33: 0.1750440891353191\n",
      "34: 0.1595211935069631\n",
      "35: 0.1451107891168075\n",
      "36: 0.13157662851520677\n",
      "37: 0.11926922168397738\n",
      "38: 0.10813173741396781\n",
      "39: 0.0980025460562318\n",
      "40: 0.08898990292952837\n",
      "41: 0.08077313591426347\n",
      "42: 0.07354975065505452\n",
      "43: 0.06693563755795093\n",
      "44: 0.061150447657549296\n",
      "45: 0.05606443254257169\n",
      "46: 0.05168344487584754\n",
      "47: 0.047765437919488736\n",
      "48: 0.04439333975805938\n",
      "49: 0.04123300084078022\n",
      "50: 0.038197174664810304\n",
      "51: 0.035487837634614425\n",
      "52: 0.03278237397976419\n",
      "53: 0.030471080052673957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m kl_loss\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m encoder\u001b[39m.\u001b[39mcleargrads()\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:130\u001b[0m, in \u001b[0;36mVariable.backward\u001b[0;34m(self, retain_grad, create_graph)\u001b[0m\n\u001b[1;32m    127\u001b[0m gys \u001b[39m=\u001b[39m [output()\u001b[39m.\u001b[39mgrad \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39moutputs]  \u001b[39m# output is weakref\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mwith\u001b[39;00m using_config(\u001b[39m\"\u001b[39m\u001b[39menable_backprop\u001b[39m\u001b[39m\"\u001b[39m, create_graph):\n\u001b[0;32m--> 130\u001b[0m     gxs \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49mgys)\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(gxs, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    132\u001b[0m         gxs \u001b[39m=\u001b[39m (gxs,)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:257\u001b[0m, in \u001b[0;36mMul.backward\u001b[0;34m(self, gy)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, gy):\n\u001b[1;32m    256\u001b[0m     x0, x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\n\u001b[0;32m--> 257\u001b[0m     gx0 \u001b[39m=\u001b[39m gy \u001b[39m*\u001b[39;49m x1\n\u001b[1;32m    258\u001b[0m     gx1 \u001b[39m=\u001b[39m gy \u001b[39m*\u001b[39m x0\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m x0\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m x1\u001b[39m.\u001b[39mshape:  \u001b[39m# for broadcast\u001b[39;00m\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:267\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x0, x1)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(x0, x1):\n\u001b[1;32m    266\u001b[0m     x1 \u001b[39m=\u001b[39m as_array(x1, dezero\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_array_module(x0\u001b[39m.\u001b[39mdata))\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m Mul()(x0, x1)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m inputs \u001b[39m=\u001b[39m [as_variable(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[1;32m    206\u001b[0m xs \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[0;32m--> 207\u001b[0m ys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mxs)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ys, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    209\u001b[0m     ys \u001b[39m=\u001b[39m (ys,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    cnt_loss = 0\n",
    "    for iter in range(strokes.max_iter):\n",
    "        x, t = strokes.__next__()\n",
    "        encoder.lstm.reset_state()\n",
    "        z, mu, sigma = encoder(x)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "        loss = kl_loss\n",
    "\n",
    "        encoder.cleargrads()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.update()\n",
    "        \n",
    "        loss = loss.data\n",
    "        print(f\"{iter}: {loss}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch} {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, cnt_loss: 7.718067034973148, loss.data: 7.718067034973148\n",
      "2, cnt_loss: 15.379087208139683, loss.data: 7.661020173166534\n",
      "3, cnt_loss: 23.136085765710728, loss.data: 7.756998557571044\n",
      "4, cnt_loss: 30.978956945243603, loss.data: 7.842871179532875\n",
      "5, cnt_loss: 38.784900157407044, loss.data: 7.805943212163444\n",
      "6, cnt_loss: 46.48082188864835, loss.data: 7.695921731241302\n",
      "7, cnt_loss: 54.16153856348972, loss.data: 7.68071667484137\n",
      "8, cnt_loss: 61.968549409250016, loss.data: 7.807010845760295\n",
      "9, cnt_loss: 69.71782058431576, loss.data: 7.74927117506574\n",
      "10, cnt_loss: 77.57460609209588, loss.data: 7.856785507780119\n",
      "11, cnt_loss: 85.25418760593666, loss.data: 7.679581513840794\n",
      "12, cnt_loss: 93.0536994037631, loss.data: 7.799511797826434\n",
      "13, cnt_loss: 100.80278033735122, loss.data: 7.749080933588123\n",
      "14, cnt_loss: 108.56173317902511, loss.data: 7.758952841673894\n",
      "15, cnt_loss: 116.26026068090833, loss.data: 7.698527501883223\n",
      "16, cnt_loss: 123.98495692899316, loss.data: 7.7246962480848245\n",
      "17, cnt_loss: 131.63995080631992, loss.data: 7.6549938773267545\n",
      "18, cnt_loss: 139.3079597440427, loss.data: 7.668008937722802\n",
      "19, cnt_loss: 146.96873990099687, loss.data: 7.660780156954166\n",
      "20, cnt_loss: 154.66793374365713, loss.data: 7.699193842660277\n",
      "21, cnt_loss: 162.32447021044777, loss.data: 7.65653646679064\n",
      "22, cnt_loss: 169.98965622300688, loss.data: 7.665186012559116\n",
      "23, cnt_loss: 177.65810312498402, loss.data: 7.668446901977145\n",
      "24, cnt_loss: 185.26180546524185, loss.data: 7.603702340257829\n",
      "25, cnt_loss: 192.9073692164278, loss.data: 7.645563751185959\n",
      "26, cnt_loss: 200.6386763225799, loss.data: 7.731307106152079\n",
      "27, cnt_loss: 208.24819914462154, loss.data: 7.609522822041642\n",
      "28, cnt_loss: 215.86931698303093, loss.data: 7.621117838409385\n",
      "29, cnt_loss: 223.55242765652056, loss.data: 7.683110673489642\n",
      "30, cnt_loss: 231.13585879658288, loss.data: 7.583431140062312\n",
      "31, cnt_loss: 238.74079377149246, loss.data: 7.604934974909574\n",
      "32, cnt_loss: 246.3111845948801, loss.data: 7.57039082338764\n",
      "33, cnt_loss: 253.8909770519371, loss.data: 7.57979245705701\n",
      "34, cnt_loss: 261.5081419438646, loss.data: 7.617164891927451\n",
      "35, cnt_loss: 269.0887984086763, loss.data: 7.580656464811758\n",
      "36, cnt_loss: 276.51605712580243, loss.data: 7.427258717126084\n",
      "37, cnt_loss: 283.96411193000614, loss.data: 7.44805480420371\n",
      "38, cnt_loss: 291.44809150430086, loss.data: 7.48397957429474\n",
      "39, cnt_loss: 298.85530911619094, loss.data: 7.407217611890066\n",
      "40, cnt_loss: 306.36926728602475, loss.data: 7.513958169833803\n",
      "41, cnt_loss: 313.8574837985744, loss.data: 7.488216512549655\n",
      "42, cnt_loss: 321.27055867628815, loss.data: 7.41307487771375\n",
      "43, cnt_loss: 328.6418323618565, loss.data: 7.3712736855683545\n",
      "44, cnt_loss: 336.0481039222039, loss.data: 7.406271560347434\n",
      "45, cnt_loss: 343.3565648895538, loss.data: 7.308460967349928\n",
      "46, cnt_loss: 350.67171862398993, loss.data: 7.315153734436121\n",
      "47, cnt_loss: 357.974455124442, loss.data: 7.302736500452033\n",
      "48, cnt_loss: 365.2585529042287, loss.data: 7.284097779786707\n",
      "49, cnt_loss: 372.6238924999912, loss.data: 7.365339595762504\n",
      "Epoch 0: 7.604569234693698\n",
      "1, cnt_loss: 7.294715359803187, loss.data: 7.294715359803187\n",
      "2, cnt_loss: 14.523659315682874, loss.data: 7.228943955879686\n",
      "3, cnt_loss: 21.82119362179024, loss.data: 7.297534306107367\n",
      "4, cnt_loss: 29.124682041205816, loss.data: 7.303488419415577\n",
      "5, cnt_loss: 36.33468771750944, loss.data: 7.210005676303621\n",
      "6, cnt_loss: 43.508431626662436, loss.data: 7.173743909152996\n",
      "7, cnt_loss: 50.714504227845346, loss.data: 7.2060726011829095\n",
      "8, cnt_loss: 57.89080104084968, loss.data: 7.176296813004336\n",
      "9, cnt_loss: 65.06544121279016, loss.data: 7.174640171940485\n",
      "10, cnt_loss: 72.05314436938363, loss.data: 6.987703156593466\n",
      "11, cnt_loss: 78.90238673424662, loss.data: 6.849242364862984\n",
      "12, cnt_loss: 85.3834120118759, loss.data: 6.481025277629288\n",
      "13, cnt_loss: 91.236238341396, loss.data: 5.852826329520101\n",
      "14, cnt_loss: 96.28038297750703, loss.data: 5.044144636111023\n",
      "15, cnt_loss: 100.47608276074422, loss.data: 4.195699783237185\n",
      "16, cnt_loss: 103.95168017224695, loss.data: 3.47559741150273\n",
      "17, cnt_loss: 106.76578483877627, loss.data: 2.8141046665293183\n",
      "18, cnt_loss: 108.96270267104397, loss.data: 2.1969178322677068\n",
      "19, cnt_loss: 110.88042899889051, loss.data: 1.9177263278465326\n",
      "20, cnt_loss: 112.59581992418953, loss.data: 1.7153909252990185\n",
      "21, cnt_loss: 114.16301653037868, loss.data: 1.5671966061891531\n",
      "22, cnt_loss: 115.65890084019964, loss.data: 1.495884309820947\n",
      "23, cnt_loss: 117.07624257582916, loss.data: 1.4173417356295155\n",
      "24, cnt_loss: 118.44366387295499, loss.data: 1.3674212971258317\n",
      "25, cnt_loss: 119.7721859227095, loss.data: 1.3285220497545087\n",
      "26, cnt_loss: 121.05128393263426, loss.data: 1.2790980099247693\n",
      "27, cnt_loss: 122.24181611381094, loss.data: 1.190532181176673\n",
      "28, cnt_loss: 123.40662573540466, loss.data: 1.1648096215937187\n",
      "29, cnt_loss: 124.53544704988191, loss.data: 1.1288213144772465\n",
      "30, cnt_loss: 125.59901497093732, loss.data: 1.0635679210554028\n",
      "31, cnt_loss: 126.64125322604534, loss.data: 1.0422382551080194\n",
      "32, cnt_loss: 127.60428504997475, loss.data: 0.9630318239294147\n",
      "33, cnt_loss: 128.5323943236973, loss.data: 0.9281092737225669\n",
      "34, cnt_loss: 129.4138792288936, loss.data: 0.8814849051962778\n",
      "35, cnt_loss: 130.24103102165387, loss.data: 0.8271517927602736\n",
      "36, cnt_loss: 131.0494026773151, loss.data: 0.8083716556612377\n",
      "37, cnt_loss: 131.79369876978765, loss.data: 0.7442960924725427\n",
      "38, cnt_loss: 132.48772599311096, loss.data: 0.6940272233232948\n",
      "39, cnt_loss: 133.16047277827042, loss.data: 0.6727467851594602\n",
      "40, cnt_loss: 133.77546318979347, loss.data: 0.6149904115230435\n",
      "41, cnt_loss: 134.36513314387784, loss.data: 0.5896699540843584\n",
      "42, cnt_loss: 134.89514089067058, loss.data: 0.5300077467927222\n",
      "43, cnt_loss: 135.40000017286062, loss.data: 0.5048592821900366\n",
      "44, cnt_loss: 135.84295827019423, loss.data: 0.442958097333608\n",
      "45, cnt_loss: 136.26271796460904, loss.data: 0.41975969441480365\n",
      "46, cnt_loss: 136.65036247652054, loss.data: 0.38764451191149085\n",
      "47, cnt_loss: 137.01343054398643, loss.data: 0.36306806746589515\n",
      "48, cnt_loss: 137.32489587688036, loss.data: 0.3114653328939401\n",
      "49, cnt_loss: 137.61460100334114, loss.data: 0.28970512646076196\n",
      "50, cnt_loss: 137.87326749756681, loss.data: 0.25866649422568083\n",
      "51, cnt_loss: 138.09094319106342, loss.data: 0.21767569349660792\n",
      "52, cnt_loss: 138.28742148539664, loss.data: 0.19647829433321531\n",
      "53, cnt_loss: 138.45741879833542, loss.data: 0.16999731293877907\n",
      "54, cnt_loss: 138.592687555004, loss.data: 0.13526875666857935\n",
      "55, cnt_loss: 138.70181552759138, loss.data: 0.10912797258736448\n",
      "56, cnt_loss: 138.7782188060939, loss.data: 0.07640327850251616\n",
      "57, cnt_loss: 138.82758513929508, loss.data: 0.04936633320118369\n",
      "58, cnt_loss: 138.861271139476, loss.data: 0.03368600018092818\n",
      "59, cnt_loss: 138.86095542877257, loss.data: -0.0003157107034421347\n",
      "60, cnt_loss: 138.85639712900263, loss.data: -0.00455829976992958\n",
      "61, cnt_loss: 138.83276986821565, loss.data: -0.023627260786974913\n",
      "62, cnt_loss: 138.77744463154545, loss.data: -0.05532523667019204\n",
      "63, cnt_loss: 138.6981649592158, loss.data: -0.07927967232965571\n",
      "64, cnt_loss: 138.58579515734127, loss.data: -0.11236980187454952\n",
      "65, cnt_loss: 138.46464252139768, loss.data: -0.12115263594359794\n",
      "66, cnt_loss: 138.33062749157787, loss.data: -0.13401502981981095\n",
      "67, cnt_loss: 138.17673047593783, loss.data: -0.153897015640045\n",
      "68, cnt_loss: 137.98620803360146, loss.data: -0.1905224423363787\n",
      "69, cnt_loss: 137.79203877587693, loss.data: -0.19416925772452115\n",
      "70, cnt_loss: 137.58132717424468, loss.data: -0.2107116016322526\n",
      "Epoch 1: 1.9654475310606383\n",
      "1, cnt_loss: -0.21364484809734174, loss.data: -0.21364484809734174\n",
      "2, cnt_loss: -0.46444378816316956, loss.data: -0.2507989400658278\n",
      "3, cnt_loss: -0.7358574479276916, loss.data: -0.271413659764522\n",
      "4, cnt_loss: -1.0234589219717312, loss.data: -0.28760147404403946\n",
      "5, cnt_loss: -1.3252991486664876, loss.data: -0.3018402266947564\n",
      "6, cnt_loss: -1.6322088344551071, loss.data: -0.30690968578861944\n",
      "7, cnt_loss: -1.9618385323976093, loss.data: -0.3296296979425021\n",
      "8, cnt_loss: -2.318827634865297, loss.data: -0.35698910246768767\n",
      "9, cnt_loss: -2.6714925150704216, loss.data: -0.35266488020512443\n",
      "10, cnt_loss: -3.042505812664192, loss.data: -0.37101329759377044\n",
      "11, cnt_loss: -3.441720952300943, loss.data: -0.39921513963675087\n",
      "12, cnt_loss: -3.8074915389372275, loss.data: -0.36577058663628437\n",
      "13, cnt_loss: -4.216919931271755, loss.data: -0.409428392334527\n",
      "14, cnt_loss: -4.631130509047939, loss.data: -0.41421057777618386\n",
      "15, cnt_loss: -5.075224965959005, loss.data: -0.444094456911066\n",
      "16, cnt_loss: -5.5297232514638806, loss.data: -0.45449828550487614\n",
      "17, cnt_loss: -5.994961303825049, loss.data: -0.4652380523611685\n",
      "18, cnt_loss: -6.485452408620867, loss.data: -0.4904911047958183\n",
      "19, cnt_loss: -6.971109044529725, loss.data: -0.48565663590885777\n",
      "20, cnt_loss: -7.487951360712199, loss.data: -0.5168423161824738\n",
      "21, cnt_loss: -8.005127067704441, loss.data: -0.5171757069922432\n",
      "22, cnt_loss: -8.517399040878201, loss.data: -0.5122719731737606\n",
      "23, cnt_loss: -9.039801204085368, loss.data: -0.5224021632071673\n",
      "24, cnt_loss: -9.584173377880449, loss.data: -0.544372173795081\n",
      "25, cnt_loss: -10.127791251907675, loss.data: -0.5436178740272263\n",
      "26, cnt_loss: -10.680070220109227, loss.data: -0.552278968201552\n",
      "27, cnt_loss: -11.271479692163936, loss.data: -0.5914094720547093\n",
      "28, cnt_loss: -11.848418198345456, loss.data: -0.5769385061815199\n",
      "29, cnt_loss: -12.436152173283599, loss.data: -0.5877339749381422\n",
      "30, cnt_loss: -13.0479735865735, loss.data: -0.6118214132899007\n",
      "31, cnt_loss: -13.667009244749373, loss.data: -0.6190356581758738\n",
      "32, cnt_loss: -14.305969202688265, loss.data: -0.6389599579388918\n",
      "33, cnt_loss: -14.960145916305011, loss.data: -0.654176713616745\n",
      "34, cnt_loss: -15.620155535436586, loss.data: -0.6600096191315744\n",
      "35, cnt_loss: -16.290880490195907, loss.data: -0.6707249547593206\n",
      "36, cnt_loss: -16.94290910231036, loss.data: -0.6520286121144546\n",
      "37, cnt_loss: -17.620860870345336, loss.data: -0.6779517680349778\n",
      "38, cnt_loss: -18.31381313405428, loss.data: -0.6929522637089423\n",
      "39, cnt_loss: -18.989395141817454, loss.data: -0.6755820077631745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m encoder\u001b[39m.\u001b[39mcleargrads()\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m decoder\u001b[39m.\u001b[39mcleargrads()\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:130\u001b[0m, in \u001b[0;36mVariable.backward\u001b[0;34m(self, retain_grad, create_graph)\u001b[0m\n\u001b[1;32m    127\u001b[0m gys \u001b[39m=\u001b[39m [output()\u001b[39m.\u001b[39mgrad \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39moutputs]  \u001b[39m# output is weakref\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mwith\u001b[39;00m using_config(\u001b[39m\"\u001b[39m\u001b[39menable_backprop\u001b[39m\u001b[39m\"\u001b[39m, create_graph):\n\u001b[0;32m--> 130\u001b[0m     gxs \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49mgys)\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(gxs, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    132\u001b[0m         gxs \u001b[39m=\u001b[39m (gxs,)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/functions.py:384\u001b[0m, in \u001b[0;36mLinear.backward\u001b[0;34m(self, gy)\u001b[0m\n\u001b[1;32m    382\u001b[0m x, W, b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\n\u001b[1;32m    383\u001b[0m gb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m b\u001b[39m.\u001b[39mdata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sum_to(gy, b\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 384\u001b[0m gx \u001b[39m=\u001b[39m matmul(gy, W\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    385\u001b[0m gW \u001b[39m=\u001b[39m matmul(x\u001b[39m.\u001b[39mT, gy)\n\u001b[1;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m gx, gW, gb\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:172\u001b[0m, in \u001b[0;36mVariable.T\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mT\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m dezero\u001b[39m.\u001b[39;49mfunctions\u001b[39m.\u001b[39;49mtranspose(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/functions.py:212\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(x, axes)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose\u001b[39m(x, axes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m Transpose(axes)(x)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/core.py:207\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    204\u001b[0m inputs \u001b[39m=\u001b[39m [as_variable(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[1;32m    206\u001b[0m xs \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs]\n\u001b[0;32m--> 207\u001b[0m ys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mxs)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ys, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    209\u001b[0m     ys \u001b[39m=\u001b[39m (ys,)\n",
      "File \u001b[0;32m~/sketch-rnn/dezero/functions.py:199\u001b[0m, in \u001b[0;36mTranspose.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 199\u001b[0m     y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mtranspose(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes)\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    cnt = 0\n",
    "    cnt_loss = 0\n",
    "    for x, t in strokes:\n",
    "        encoder.lstm.reset_state()\n",
    "        decoder.lstm.reset_state()\n",
    "        z, mu, sigma = encoder(x)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        xp = cuda.get_array_module(z)\n",
    "\n",
    "        expanded_z = F.expand_dims(z, axis=1)\n",
    "        \n",
    "        z_stack = None\n",
    "        \n",
    "        for i in range(seq_len - 1):\n",
    "            if i == 0:\n",
    "                z_stack = expanded_z\n",
    "            else:\n",
    "                z_stack = F.cat((z_stack, expanded_z), axis=1)\n",
    "        # x = F.expand_dims(z, axis=1)\n",
    "\n",
    "        inputs = F.cat((x[:,:-1], z_stack), axis=2)\n",
    "        # inputs = dezero.as_variable(inputs)\n",
    "        bgm, q_logits, _, _ = decoder(inputs, z)\n",
    "\n",
    "        kl_loss = KLDivergenceLoss(mu, sigma)\n",
    "        rec_loss = ReconstructionLoss(t, x[:,1:], bgm, q_logits)\n",
    "        loss = kl_loss + rec_loss\n",
    "\n",
    "        encoder.cleargrads()\n",
    "        decoder.cleargrads()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.update()\n",
    "        decoder_optimizer.update()\n",
    "        \n",
    "        cnt_loss += loss.data\n",
    "        cnt+=1\n",
    "        print(f\"{cnt}, cnt_loss: {cnt_loss}, loss.data: {loss.data}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch}: {cnt_loss/cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def sample(self, x, temperature=1.0):\n",
    "        # x is a batch of stroke data\n",
    "        longest_seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        z, _, _ = self.encoder(x)\n",
    "        xp = cuda.get_array_module(x)\n",
    "        \n",
    "        s = xp.zeros(5)\n",
    "        # mark the start of the sequence\n",
    "        s[2] = 1\n",
    "        \n",
    "        print(s)\n",
    "        # s is the initial stroke\n",
    "        # we are going to create a sequence of strokes\n",
    "        # we first \"\"\n",
    "        seq = xp.array([s])\n",
    "        \n",
    "        h = None\n",
    "        c = None\n",
    "        \n",
    "        \n",
    "        with dezero.no_grad():\n",
    "            for i in range(longest_seq_len):\n",
    "                expanded_z = F.expand_dims(z, axis=1)\n",
    "                # change s to a 3D array to have (1,1,)\n",
    "                \n",
    "                morphed_s = F.reshape(s, (1, 1, -1))\n",
    "                \n",
    "                stacked_s = None\n",
    "                for i in range(batch_size):\n",
    "                    if stacked_s is None:\n",
    "                        stacked_s = morphed_s\n",
    "                    else:\n",
    "                        stacked_s = F.cat((stacked_s, morphed_s), axis=0)\n",
    "                        \n",
    "                \n",
    "                # print(stacked_s.shape, expanded_z.shape, \"stacked_s, expanded_z\")\n",
    "                inputs = F.cat((stacked_s, expanded_z), axis=2)\n",
    "                # 여기 아래 부분에서 오류가 생긴다\n",
    "                \n",
    "                if h is None and c is None:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z)\n",
    "                else:\n",
    "                    bgm, q_logits, h, c = self.decoder(inputs, z, h, c)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                s = self._sample_step(bgm, q_logits, temperature)\n",
    "                \n",
    "                xp.append(seq, s)\n",
    "                # seq.append(s)\n",
    "                \n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "                \n",
    "        seq = F.cat(seq, axis=1)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "                \n",
    "    def _sample_step(self, bgm, q_logits, temperature):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        bgm.set_temperature(temperature)\n",
    "        \n",
    "        # pring if bgm pi sum is 1\n",
    "        \n",
    "        # print(bgm.pi_logits.shape, 'pi logits shape in sample step')\n",
    "        seq_len = bgm.pi_logits.shape[1]\n",
    "        # print(F.reshape(F.sum(bgm.pi_logits, axis=2), (-1, seq_len)), 'sum of pi _sample_step')\n",
    "        gen_result = bgm.sample(1)\n",
    "        \n",
    "        # we need categorical distribution q for pen state\n",
    "        # convert scaled logits to probabilities\n",
    "        probabilities = F.softmax(q_logits / temperature, axis=-1)\n",
    "        \n",
    "        gen_x = gen_result[:, :, 0]\n",
    "        gen_y = gen_result[:, :, 1]\n",
    "        \n",
    "        # sample from categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        xp = cuda.get_array_module(gen_x)\n",
    "        stroke = xp.zeros(5, dtype=q_logits.dtype)\n",
    "        \n",
    "        \n",
    "        print(gen_x.shape, gen_y.shape, probabilities, \"gen_x, gen_y, probabilities\")\n",
    "        # print(stroke, \"stroke\")\n",
    "        # fill in gen_x and gen_y\n",
    "        # print(gen_x[0][0], gen_y[0][0])\n",
    "        stroke[0] = gen_x[0][0]\n",
    "        stroke[1] = gen_y[0][0]\n",
    "        \n",
    "        # sample from the categorical distribution\n",
    "        # we need to sample from the pen state\n",
    "        print(probabilities[0][0].data, 'probabilities')\n",
    "        # choose pen state from 0, 1, 2\n",
    "        pen = xp.random.choice(3, 1, p=probabilities[0][0].data)\n",
    "        print(pen)\n",
    "        \n",
    "        stroke[2 + pen] = 1\n",
    "        \n",
    "        return stroke\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (1000, 64, 5)\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'multiply'>, '__call__', array([0.03604792, 0.736856  ]), [variable(0.44269818748653245), variable(0.6282123466995799)]): 'ndarray', 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# add dimension to make it compatible with the model, batch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minput shape: \u001b[39m\u001b[39m\"\u001b[39m,batch_data\u001b[39m.\u001b[39mshape) \u001b[39m# we have one batch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sample \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(batch_data, temperature\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m)\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     bgm, q_logits, h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(inputs, z, h, c)\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_step(bgm, q_logits, temperature)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m xp\u001b[39m.\u001b[39mappend(seq, s)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# seq.append(s)\u001b[39;00m\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m seq_len \u001b[39m=\u001b[39m bgm\u001b[39m.\u001b[39mpi_logits\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# print(F.reshape(F.sum(bgm.pi_logits, axis=2), (-1, seq_len)), 'sum of pi _sample_step')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m gen_result \u001b[39m=\u001b[39m bgm\u001b[39m.\u001b[39;49msample(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# we need categorical distribution q for pen state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# convert scaled logits to probabilities\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m probabilities \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(q_logits \u001b[39m/\u001b[39m temperature, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/elicer/sketch-rnn/vae_test.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m rho \u001b[39m=\u001b[39m rho_xy[i, idx]\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m# print(mu + rn[i, j] * std, 'mu + rn[i, j] * std')\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m result_x_y \u001b[39m=\u001b[39m (mu \u001b[39m+\u001b[39m rn[i, j] \u001b[39m*\u001b[39;49m std)\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m# print(result_x_y[0].data, 'this is resuult')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://xscnjbqhnyrrrkmh.tunnel-pt.elice.io/home/elicer/sketch-rnn/vae_test.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m result[i, j, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m result_x_y[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1285\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__mul__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: operand type(s) all returned NotImplemented from __array_ufunc__(<ufunc 'multiply'>, '__call__', array([0.03604792, 0.736856  ]), [variable(0.44269818748653245), variable(0.6282123466995799)]): 'ndarray', 'list'"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(encoder, decoder)\n",
    "\n",
    "batch_data = x[0:batch_size] # since we trained with batch size of 4, we can only sample 4 sketches\n",
    "# add dimension to make it compatible with the model, batch\n",
    "print(\"input shape: \",batch_data.shape) # we have one batch\n",
    "\n",
    "sample = sampler.sample(batch_data, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
